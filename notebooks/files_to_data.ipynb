{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import img_to_array, load_img\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import json\n",
    "import utils\n",
    "\n",
    "CONFIG = utils.load_config()\n",
    "IMAGES_PATH = CONFIG[\"images_path\"]\n",
    "CONFIG_FILE = \"../config.json\"\n",
    "IMAGE_DIMENSIONS = (299, 299)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_images(image_paths: pathlib.Path, target_size=IMAGE_DIMENSIONS) -> np.ndarray:\n",
    "    out = []\n",
    "    for p in image_paths:\n",
    "        img = load_img(p, target_size=target_size)\n",
    "        arr = img_to_array(img) / 255.0\n",
    "        out.append(arr)\n",
    "    return np.array(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = [\n",
    "    \"flickr_scraper/images/1900/190756525.jpg\",\n",
    "]\n",
    "\n",
    "img = load_img(image_paths[0])\n",
    "arr = img_to_array(img)\n",
    "arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "\n",
    "def load_data_for_year(dir: pathlib.Path, target_size=(224,224)) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads images and labels for a year.\n",
    "\n",
    "    Args:\n",
    "        dir : pathlib.Path\n",
    "            Directory containing images and a metadata.csv file.\n",
    "\n",
    "        target_size : tuple[int, int],  default=(224,224)\n",
    "            Size to resize images to (height, width).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray\n",
    "            Array of preprocessed images.\n",
    "\n",
    "        np.ndarray\n",
    "            Array of labels (year, latitude, longitude).\n",
    "    \"\"\"\n",
    "    if isinstance(dir, str):\n",
    "        dir = pathlib.Path(dir)\n",
    "    \n",
    "    df = pl.read_csv(dir / \"metadata.csv\")\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for id in df[\"id\"]:\n",
    "        filename = str(id) + \".jpg\"\n",
    "        try:\n",
    "            img = load_img(dir / filename, target_size=target_size)\n",
    "            arr = img_to_array(img) / 255.0\n",
    "            images.append(arr)\n",
    "\n",
    "            # Saves metadata of file\n",
    "            metadata = df.filter(pl.col(\"id\") == id).select([\"year\", \"latitude\", \"longitude\"])\n",
    "            labels.append(metadata.to_numpy()[0])\n",
    "        except FileNotFoundError:\n",
    "            continue\n",
    "\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "images, labels = load_data_for_year(pathlib.Path(IMAGES_PATH + \"/1900\"))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "def load_all_data(dir: pathlib.Path, target_size=(224, 224), scaling=True, start_year=1900, end_year=2025) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Loads images and labels from a directory.\n",
    "\n",
    "    Args:\n",
    "        dir : pathlib.Path\n",
    "            Directory year subdirectories.\n",
    "\n",
    "        target_size : tuple[int, int],  default=(224,224)\n",
    "            Size to resize images to (height, width).\n",
    "\n",
    "        scaling : bool, default=True\n",
    "            Whether or not to scale coordinates.\n",
    "\n",
    "        start_year : int, default=1900\n",
    "            First year of the range to include images from (inclusive).\n",
    "\n",
    "        end_year : int, default=2025\n",
    "            Last year of the range to include images from (exclusive).\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray\n",
    "            array of preprocessed images\n",
    "\n",
    "        np.ndarray\n",
    "            array of labels (year, latitude, longitude)\n",
    "    \"\"\"\n",
    "    if isinstance(dir, str):\n",
    "        dir = pathlib.Path(dir)\n",
    "\n",
    "    images = []\n",
    "    labels = []\n",
    "    for d in sorted(dir.iterdir()):\n",
    "        if int(d.name) < start_year:\n",
    "            continue\n",
    "        elif int(d.name) >= end_year:\n",
    "            break\n",
    "        new_images, new_labels = load_data_for_year(d, target_size=target_size)\n",
    "        images.append(new_images)\n",
    "        labels.append(new_labels)\n",
    "    image_arr = np.concatenate(images)\n",
    "    label_arr = np.concatenate(labels)\n",
    "\n",
    "    if scaling:\n",
    "        scaler = MinMaxScaler()\n",
    "        label_arr[:, 1:] = scaler.fit_transform(label_arr[:, 1:])\n",
    "\n",
    "    return image_arr, label_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = load_all_data(IMAGES_PATH, start_year=1945, end_year=1960)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "FAULTY_YEAR = IMAGES_PATH + \"/1914\"\n",
    "METADATA_FILE = FAULTY_YEAR + \"/metadata.csv\"\n",
    "\n",
    "file_count = len([f for f in os.listdir(FAULTY_YEAR) if os.path.isfile(os.path.join(FAULTY_YEAR,f))])\n",
    "df = pl.read_csv(METADATA_FILE).unique([\"id\"]) #, \"latitude\", \"longitude\"])\n",
    "print(file_count)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "faulty_dir = pathlib.Path(FAULTY_YEAR)\n",
    "\n",
    "downloaded_ids = []\n",
    "for f in faulty_dir.glob(\"*.jpg\"):\n",
    "    downloaded_ids.append(int(f.name.replace(\".jpg\", \"\")))\n",
    "\n",
    "ids_without_metadata = []\n",
    "for id in downloaded_ids:\n",
    "    if id not in df[\"id\"]:\n",
    "        ids_without_metadata.append(id)\n",
    "ids_without_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = pathlib.Path(FAULTY_YEAR)\n",
    "d.name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
